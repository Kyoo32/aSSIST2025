## Lecture 1 Vectors

- **Vectors and Matrices** are ways to organize data into rows and columns. A **Tensor** is similar to a matrix but just has more indices, like a cube of numbers instead of a flat square.
    
- A **matrix** can be understood as a **linear transformation** that acts on a vector within a vector space.
    
    > **In simple terms:** A matrix is like an Instagram filter 📸. It takes your original vector (the photo) and transforms it into a new one by rotating, stretching, or flipping it.
    
- A **norm** is a function that allows us to calculate the length and distance of vectors in that space.
    
    > **In simple terms:** A norm just answers the question, "How long is this vector?"
    > 
    > - **Euclidean Norm(L2 Norm)** is the "as the crow flies" straight-line distance 🐦.
    >     
    > - **Manhattan Norm(L1 Norm)** is the "taxicab" distance, moving only along a grid 🚕.
    >     
    > - **Infinity Norm** is the "chessboard" distance, finding the longest single move in any one direction ♟️.
    >     
    
- **Vectors** are used directly in Large Language Models (LLMs) in the form of **embeddings**.

